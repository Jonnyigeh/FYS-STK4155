\documentclass[../main.tex]{subfiles}

\begin{document}
\newpage
\section{Discussion}
\subsection{Ordinary Least Squares}
From the initial plot in figure \eqref{fig:franke_5th_poly} we can see that using OLS with a 5th order polynomial reproduces the surface somewhat. However, just studying the plots are not enough to deduce if the model is actually a good fit or not. This is especially true when working in multiple dimensions (if we go even higher, plotting will not even be possible!), and as such we must calculate the various error estimates that we've presented in the result section. \\ \\ \indent One thing to note before delving into the interpretation of the various results obtained using the Ordinary Least Squares is that we've not scaled any of the data used within this method. \cite{Linear_regression}This is due to the OLS method being scale equivariant and as such scaling the data will only scale the fit linearly - and the actual model fit will be the same by a scaling factor - meaning we can disregard this, to save on the computational load. This load may very well be quite large when working with real datasets.
\subsubsection{MSE and R$^2$}
The simplest way to check if our model fits the data nicely, is to study the error between the model and data. We do this by checking MSE and R2 score - as we've done in figure \eqref{fig:MSE_MSEtrain_R2_score}. What we are looking to find, is a point where the R2 score is maximized while the MSE is minimized. This seems to occur at polynomial degree 4. Since we've used the Franke Function\footnote{See: \url{https://www.sfu.ca/~ssurjano/franke2d.html}} to produce our dataset this should make sense that a higher order polynomial produces the best results.
\\ \\ \indent When looking closer at the relationship between the MSE produced from using the train and test data respectively we see that these deviate greatly for lower polynomial degrees, but then they tend similar values for higher orders. One would expect the train MSE to also be quite large at first order - and this could indicate that we've overfitted the data in this particular example. The large deviance between train and test MSE also indicates areas of high variance. As we've mentioned in the theory section (see \eqref{refmethod:Bias_variance_decomp}) a high variance in our model means that the model varies greatly when we change the input data, i.e when we've split our dataset according to \eqref{refmethod:train_test} and evaluate our model on train and test data respectively, a high variance will naturally produce a high discrepancy between the two estimates, while a low variance would not.

\\\\\indent And as seen from the Bias-Variance decomposition of the MSE (see \eqref{app:bias_variance_decomp}), we know that a high variance should correspond to a low bias (unless the MSE is extremely high, but in relative magnitude these two contributions should oppose eachother) - this flows naturally into our argument that the model varies greatly for different datasets with high variance - because a high bias would then correspond to the models attraction to produce the same data for any data set, i.e low variance just as we've proven with the bias-variance decomposition.

\subsubsection{Variance of parameter $\beta$}\label{Discussion:Variance_of_beta}
In these results shown in figure(\ref{fig: Variance_parameter_beta}) one can see a clear change in variance of parameter $\beta$ especially when we increase to a higher order of features. This makes a lot of meaning aswell since when the features increases our degrees of polynomial increases as well and that gives a higher order of spread in our data set. And when we have higher order of spread in data, the larger the variance is in relation to the mean.
\vskip0.1in
But we can also see in figure(\ref{fig: Variance_parameter_beta}) that the variance of parameter $\beta$ doesn't always necessary increase when the complexity increases. The complexity increases when the features is set to be more complex. This is given when the set of features have a higher degree of difference. We can see that the complexity increases when we have a greater degree of difference in the exponent of these variables in each set of features. By this I mean that if $X$ and $Y$ have a large degree of inequality in the exponent, i.e $[x^4y^6,x^6y^4,x^5y^5]$ etc, then the complexity will increase effectively much more than if there was not such a large difference in the exponent of $X$ and $Y$, thus giving higher variance, which is proven to  be true in our case and equivalent to figure(\ref{fig: Variance_parameter_beta})

\subsubsection{Bias-variance tradeoff}
In these results we can see a clear pattern recognition between figure(\ref{fig: bias_variance_tradeoff_N30_N50}a) and figure(\ref{fig:biasvartradeoff}), where we see that once there is an increase in variance there is also a decrease in bias, which we can also look at when we examine the \eqref{app:bias_variance_decomp}. This can be interpreted in (\ref{fig:biasvartradeoff}), where we obtained precise data results.
\vskip0.1in 

\begin{flushleft}
 Further we can also observe that the bias and variance changes from 5th to 25th order of polynomial degree. That is also due to the changes of complexity, which is also very good data results, and the reason of this were discussed earlier when we were looking at the variance of
 parameter$\beta$\ref{Discussion:Variance_of_beta}. We can also see that the MSE increases severely due to the changes in bias and variance, which is also set to be proven in \eqref{app:bias_variance_decomp} and this follows the increasing mean squared error, which is very clear. Once the polynomial degree increases we gain a higher order of complexity within our model, this is very accurate due to the higher order of features and hence the degree of spread in our data set.
\end{flushleft}

\newpage
\subsubsection{Cross validation vs Bootstrap}
When performing linear regression and training a model against a data set of our own making we are free to produce as much data as we want, i.e we can increase number of datapoints until our computational load hits the moon. This is not the case when we are studying real life problems, in these problems the data are limited, and thus producing a proper fit may prove difficult. This is why we are to use the resampling methods introduced in the theory-method section (see \eqref{refmethod:resampling}), to produce "new" datasets from our existing sets.  \\ \\
\indent Another feature of resampling is to guarantee that we train our model using as much of the available dataset as possible, exactly for the reason mentioned: We have limited data, so we better use it efficiently. As explained in the methodsection \eqref{refmethod:resampling}, these two methods tackle this issue in different ways. While the Bootstrap method keeps the test dataset completely separate and produce "new" training data from the existing training set, the crossvalidation method splits the data into folds, and lets every fold get its turn as a test set, no discrimination whatsoever. As they tackle the same issue in separate ways, we expect them to produce similar results, but not necessarily the same. For the crossvalidation method we can tweak the number of folds, while bootstrap tweaks the number of resampling iterations.  \\ \\\indent Looking at the illustration in figure \eqref{fig:CV_MSE_kFOLD_N50}, ran for a set number of bootstrap iterations (25), the crossvalidation method becomes equally good, and even slightly better for some polynomial fits when we pass the $k=10$ folds. Exactly why this is, has to do with the size of our dataset. We've produced a grid of data points $50x50$, and if we increase the number of folds we will fit the data extremely well due to each fold being a narrower portion of the dataset. If you've 10 datapoints and 10 fold, then you fit the data 10 times on each data point - naturally this will fit your data extremely well - however, we should be on the lookout for overfitting of the data, if you make too many folds on a very small dataset, the noise will make a greater contribution to the model fit and we run into the problem of overfitting.




\subsection{Ridge regression}
Now that we move onto Ridge Regression we need to evaluate the pre-processing of our data. As opposed to the necessity of data scaling when working with the OLS, the Ridge Regression method \emph{is} affected substantially when we scale our data - and it is indeed very important that we do so. We will use the mean normalization method as presented in section \eqref{refmethod:MEAN_and_SCALING}, and the reason for this can be seen directly from the cost function for the Ridge Regression (see \eqref{eq:cost_func_ridge}), that the parameters $\beta_i$ all contribute to the MSE, with the same $\lambda$, as explained in \eqref{refmethod:MEAN_and_SCALING}
\subsubsection{Bias-Variance tradeoff}
As we did with the Ordinary Least Squares we perform the linear regression using Bootstrap resampling to provide more accurate estimates of our desired estimands, namely the MSE, Bias and variance. In figure \eqref{fig:CV_MSE_kFOLD_N50}) we've performed this analysis for 3 selected values for lambda. What we expect to find is that for an increase in $\lambda$, we should see a decrease in the models variance. This can be seen mathematically from the analytical expression for the parameters $\beta$ in eq. \eqref{}. \\\\\indent In the figure this may not be overwhelmingly clear, but as we rise in value of $\lambda$ we can see that the bias line edges closer and closer to the MSE line. As we remember from the Bias-Variance decomposition, this must mean that the variance is getting smaller. This is our desired, and expected result - a higher value of lambda means that the $\lambda\mathbf{I}$ term will dominate when determining the parameters $\beta$, and thus changing the features inside $\mathbf{X}$ will contribute less - i.e vary the model less (meaning variance is low).


\subsubsection{Cross validation vs. bootstrap}
Again we will study how the various resampling methods affect our new linear regression model. We would expect to see much similar results in this, like we did for the ordinary least squares (see \eqref{fig:CV_MSE_kFOLD_N50}) - but this time we illustrate this as a function of the new penalty parameter $\lambda$, for 3 different polynomials, as seen in figure \eqref{fig: CV_Bootstrap_RIDGE}.
\\\\\indent Again, we'd expect the two methods to coincide when we increase the number of folds (since we run for a rather high number of bootstrap for such a small dataset). But the interesting part is how the MSE changes for higher values of $\lambda$ in this analysis. We can see that in the 5th order polynomial bootstrap gives us a minima around $\log_{10}(\lambda)=-1$, while our crossvalidation method indicates this to be a maxima. However, this may come from the bootstrap method being very variant - i.e it jumps up and down quite much, so if we exclude the outliers in the x-axis it seems that the two methods coincide quite well for the higher values of $k$. What we mean by this, is that the overrall \emph{shape} of the two curves have similar features around this value for $\lambda$.

\subsection{LASSO}  
We do the same analysis for LASSO regression as we did for the Ridge Regression. Here we also need to perform mean normalization of our data in order to get a proper model fit, and for our MSE to not be evaluated wrongly due to unscaled terms (as mentioned this is not such a big issue for us, but we still need to perform the scaling to get proper assessment of our model). 
\subsubsection{Bias-Variance tradeoff}
When we study the Bias-Variance of the Lasso regression method we find much of the same that we did for the Ridge Regression analysis. As a matter of fact, these two methods seems to produce almost the same results - which may not come as a surprise given that they are in fact very similar in how they define the cost functions, as we've discussed earlier (see \eqref{refmethod:LASSO}). \\ \\\indent What we see in the results from the LASSO model is that the variance is almost non-existant (zero), regardless of choice of $\lambda$. However, one can see a slight difference in MSE and Bias$^2$ for the smallest value of $\lambda$. We may deduce that our model is overfitting the data from the fact that the Bias is so large. Another possibility is that we run the rest using very few datapoints, meaning the LASSO makes an amazing fit (and the fit stays the same even when we resample, since our datapoints lie between [0,1]) regardless of the resampled data set. A third possibility is that we've chosen too large values of $\lambda$. Since a high $\lambda$ equals low variance. Knowing exactly which of these three possibilities are the reason for the shape of our graph is not clear just from reading the plots.

\subsubsection{Cross validation vs. bootstrap}
What we can see from the plots of crossvalidation versus bootstrap resampling for the LASSO regression case, presented in figure \eqref{fig: CV_Bootstrap_LASSO} - is that, as opposed to Ridge, bootstrap seems to outperform cross validation regardless of polynomial degree and for most values of $\lambda$. We do however, seem to find the same optimal value of $\lambda$ for both cross validation and bootstrap.

\subsection{Analysis of real terrain data: Stavanger}
When performing our linear regression methods on a set of real terrain data produced from the beautiful countryside of Stavanger, we see from figure \eqref{fig:realdataanalysis} that our polynomial fit is not a great success. From the y-axis you can see the immense values that the MSE takes, in the scale of $10^5$. This indicates that a 5th order polynomial is practically useless to approximate such noisy terrain data as the one we have used in this example. We can further back this statement by looking at the values presented in table \eqref{tab:realdata}.  \\ \\\indent What we can deduce however, is that even though our model is quite poor - we can see that Lasso is the best fit for our data, albeit best of the very worst. Another interesting take from our results is that we still recognise similar traits in the cross validation plots, as we did for the Franke function: Ridge and Lasso has minima for different values of lambda, and where ridge has minima - lasso has (local) maxima.
\end{document}